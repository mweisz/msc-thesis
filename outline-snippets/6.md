# Summary
- In the scope of the thesis we have investigated the problem of counterfactual inference by using neural networks
- In particular we have introduced the concepts of propensity-based dropout and 
- Experiments have shown that the DCN-PD outperforms the state-of-the-art. Multi-task framework is suitable. Propensity dropout is good (basically copy from the exp section)
- 

# Contribution
- The contributions of the thesis are two-fold
- First: Introduced DCN + Dropout scheme
- Second: Architecture Learning for 
- Improve the state-of-the-art
- 

# Limitations

## DCN-PD:
- Only works for binary case with a single treatment
- does not support time series

## DCN-LA:
- Empirical model is pr-trained on synthetic data which might look different from real-world data
- Empirical model might be too simple. What about using more features and characteristics? 
- We are currently not condersing the number of hidden units per layer and only define the learnt architecture in terms of the ratios of layers. 
