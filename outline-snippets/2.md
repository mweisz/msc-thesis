# Introduction
This chapter describes the theoretial background. 
It introduces machines learning in general, deep learning in particular discussing different types of architectures and common use techniques. 

% TODO Intro: Write introduction to chapter depending on the actual contents. 

---

# Machine Learning
In this section we will discuss the topic of machine learning in general. 
% TODO Intro: Introduction sentence 

### What is machine Learning? 
Field in computer science. 
Learning from data. 
Show diagram (model, algorithm, learning, data)
What is the motivation? 

### Types of Machine Learning
Supervised learning. Give some examples. Regression and Classification. 
Unsupervised Learning. Give some examples. 
Reinforcement Learning. Give examples.
Others: Briefly mention other types of ML. 

### Training
Typically: Gradient Descent
What types are there? 
What's the challenge? (local minimum, symmetries, etc.)


### Overfitting vs Underfitting 
Splitting the data. 
Validation set. 
Regularisation. Different types. 

### Typical Models
Linear Regression
Logistic Regression
Decision Trees, etc. 

---

# Deep Learning:
In this section we will discuss the topic of deep learning in particular. 
% TODO Intro: Introduction sentence 

### Motivation
Inspired by how the human brain works.
Capture complex correlations within the data. 
Mention imagenet.
As few assumptions as possible. 
Show graph with typical neural network
% TODO What are other motivations? 

### Perceptron 
Show graph explaining how it works
Formalisation

### MLPs
Describe back-propagation.
Non-linearities
Activation functions


### Training


### Topologies:
Fully connected Feed forward neural networks.
Convolutional Neural Networks
Recurrent Neural Networks

### Dropout:
NNs are very complex and have lots of parameters. 
Prone to over-fitting. 
Need regularisation techniques. 
Dropout is one of the strongest methods. 
Alternatives include DropConnect etc. 

### Multi-task learning:
Neural Networks can have multiple outputs. 

### Architecture Learning
It is difficult to derive proper architecture. 
What's the right number of hidden layers? What's the right number of hidden units per layer, etc? 
Different approaches to architecture learning. Optimal Brain Surgeon/Damage. GridSearch, RandomSearch, Bayesian Optimisation.

### Challenges
Computational complexity
Lots of parameters (high memory)
Lots of hyper-parameters 
Low Understandability. Some people confuse softmax with confidence.  

---

# Counterfactual Inference
In this section we will discuss the the problem of counterfactual inference.. 
% TODO Intro: Introduction sentence 

### Motivation
Increasing amount of available data. 
Want to understand causal relations in the data to make good predictions. 
Data from Observational study. Often interested in counterfactual questions because it allows us estimate ITE. 

### Formalisation
% TODO Take formalisations that I already have
The problem of evaluation

### Not supervised learning
Using a set of factual outcomes, we are making inferences over a set of counterfactual outcomes. 
They don't follow same distribution. 
Show graph to visualise this. And how the similarities change (see Sontag's Workshop PDF)

### Propensity Score
Treatment assignemnt is not random. 
Reflects underlying policy. 
Represents probability of subject to receive treatment. 
Show formalisation that I already have. 
Show graph

### Status Quo
% TODO Check out Ahmed's related work sections
Statistic Approaches
Propensity-based approaches
Lately, neural networks

### Open Questions
How to improve accuracy? 
What are good architectures? 
How to train them? 
Dropout? 
