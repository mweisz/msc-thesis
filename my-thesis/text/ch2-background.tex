%\begin{savequote}[8cm]
%\textlatin{Jedem Anfang wohnt ein Zauber innne.}
%
%In the core of every beginning lives magic.
%  \qauthor{--- Hermann Hesse's \textit{Stufen}}
%\end{savequote}

\chapter{\label{ch:2-background}Theoretical Background} 

%\minitoc

\section{Introduction}
This chapter describes the theoretical background of machine learning and counterfactual inference including their formalisation, core concepts, state-of-the-art, and challenges. 

We start by giving a general introduction to machine learning and  its core concepts. 

The second part of the chapter deals with  \emph{deep learning} -- a particular subset of machine learning that uses deep neural networks. 

The last part of the chapter is dedicated to the problem of counterfactual inference. We describe the importance of the problem and its application areas, and give a formalisation. Furthermore, we outline the challenges of the problem and the different approaches that have been applied to it so far. We conclude, by relating counterfactual inference to deep learning and describing its open research questions which is the very foundation of this thesis. 

% TODO Intro: Write introduction in more detail depending on the actual contents. 


\section{Machine Learning}


\subsection{Motivation}
When conceptualising computer programs, we often think of them as series of unambiguous, (mostly) atomic instructions that are executed in a deterministic way and have to be explicitly programmed by a human programmer. 

While certain problem areas -- in particular those for which a well-defined algorithm or effective step-by-step solution strategy exists -- can successfully expressed and ultimately solved this way, there are various problems where this seems infeasible. A typical example for the second category is autonomous driving for which there are far too many situations and eventualities to encode the desired behaviour of an autonomous vehicle as a finite sequence of conditional instructions. 

Machine learning is a subfield of computer science that deals with the question of how to teach computer programs to learn without being explicitly programmed what to do. 
% TODO Check if this it too close to the quote at wikipedia 

More formally, an algorithm "is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E". (Mitchell)
% CITE:  Mitchell, T. (1997). Machine Learning. McGraw Hill. p. 2. ISBN 0-07-042807-7.
Recalling our example of autonomous driving, for instance, we could define the task $T$ in terms of "moving the car from A to B" while our performance measure $P$ might include aspects such as the travel time and the number of people or objects harmed. The experience $E$ would consists of the \emph{training data} that is acquired by previously covered distances (either autonomously or by "observing" a human driver). 

% TODO Show diagram (model, algorithm, learning, data)

\subsection{Types of Learning}
Machine Learning is typically categorised into \emph{supervised learning}, \emph{unsupervised learning}, \emph{reinforcement learning}. 

% TODO For each of the categories, make it more formal 

\subsubsection{Supervised Learning} 
In supervised learning the task is to infer a function over a number of samples from a set of training data that is said to be \emph{labelled}. In other words, for each sample in the training data, we have access to the actual function value we want to predict. 

For instance, we might be interested predicting housing prices where we are given a dataset consisting of historic information regarding houses (e.g. number of rooms, area in $m^2$, etc.), commonly referred  to as \emph{features} or \emph{covariates}, and the actual price for which the house has been sold, called \emph{outcome}, \emph{target}, or \emph{label}. Once a certain model has been trained by using the historic labelled dataset, we can use the same model to predict the housing prices on unseen data points, i.e. new houses for which the price is unknown. Such a task of predicting a continuous variable is called a \emph{regression} task whereas the predicting of a discrete output is referred to as a \emph{classification} task. 

\subsubsection{Unsupervised Learning} 
In contrast to supervised learning, the training set in an unsupervised setting does not contain the target labels. Typical tasks in this field include finding particular patterns in the data and clustering it accordingly. It is important to note, however, that for supervised learning, there is normally no ground-truth meaning that the performance (or quality) of the outcome cannot be easily be measure or even defined in absolute terms but might depend on the underlying use-case of the clustering. Another important task in supervised learning is \emph{dimensionality reduction} for which a (in terms of the features) high-dimensional dataset is reduces to a target lower dimension while trying to minimise the information loss. 

\subsubsection{Reinforcement Learning}
In reinforcement learning, a software agent has to learn appropriate actions in a dynamic environment in which the consequences of an action might not be immediately accessible but add up to a long-term reward that ought to be maximised. 

A most prominent example was mentioned earlier with the task of autonomous driving in which a vehicle is governed by a piece of software that has to find adequate series of actions (steering, regulating speed, etc.) in order to reach a target destination in a complex and ever-changing environment. 


% TODO! Re-create graph!
\begin{figure}[]
	\caption{Schematic Approach of Machine Learning. A selected model $M$ is trained on a dataset $D$ using a learning algorithm $A$.} \label{fig:machine-learning-approach}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/chapter-2/machine-learning-approach.png}
\end{figure}
\subsection{Machine Learning Models}
As illustrated in figure \ref{fig:machine-learning-approach}, machine learning is based on the concept of selecting an appropriate model and fitting it to a given (training) data set using a training algorithm. 

For instance, we might want to use a \emph{linear model} for our task of predicting housing prices. We can then make use \emph{linear regression} which allows us to fit our linear model to our labelled dataset of existing houses and their prices. 
 
The selection of an appropriate model is of great importance and determines important factors such as the trade-off between the expressiveness (i.e. the ability to capture complex relationships) and the computational complexity of our model (i.e. how difficult it is to train it). 

There is a number of models each with individual pros and cons depending on the desired task and the desired characteristics. Typical models include \emph{linear models}, \emph{decision trees}, and \emph{neural networks}. 
% TODO What are other typical models. How do they compare against each other? 
% Should I sketch them out here? 

This dissertation focuses on using \emph{deep learning} for the problem of counterfactual inference. Therefore, a dedicated part of this chapter deals exclusively with deep neural networks and describes their characteristics in more detail. 



%\subsection{Training}
%Machine learning is based on the concept of training a selected model by using a 
%
%Typically: Gradient Descent
%
%
%What types are there? 
%What's the challenge? (local minimum, symmetries, etc.)
%% TODO Include graph

\subsection{Regularisation}
When training our model we have to find the right balance between fitting it most accurately to the training data while making sure that the model generalises will for unseen data points. 
Given that the expressiveness of the model is sufficiently high to capture it, we might naively fit our model to the training data perfectly, resulting in a training error of zero. This, however, would merely "memorise" the training data and might perform poorly on unseen future data points. This phenomenon where we overfit the training data is often referred to as \emph{high variance}. 
In contrast, if the model is too simple, we night not be able to accurately capture the relationship in our data leading to equally poor results. In this case, we are under-fitting the data and our model has a so called \emph{high bias}.
Therefore, the goal is to train the model in a way that neither overfits nor underfits and generalises well to unseen datapoints. This can be achieved by a concept called \emph{regulariation}.

% TODO Add formula for objective functions with L1 and L2 regularisation

% TODO Regularistion is actually just for avoiding overfitting!   

% TODO! Re-create graph!
\begin{figure}[]
	\label{fig:overfitting-underfitting}

	\centering
	\includegraphics[width=0.8\textwidth]{figures/chapter-2/overfitting-underfitting.png}
	\caption{Visualisation of Bias variance Tradeoff. The model on the left is ... whereas the middle one ... and the right one is ...}
\end{figure}

% TODO Should I include learning curves as well? 
%% TODO! Re-create graph!
%\begin{figure}[]
%	\label{fig:machine-learning-approach}
%	\caption{Visualisation of Learning Curves. The left model is ... whereas the right model is ...}
%	\centering
%	\includegraphics[width=0.8\textwidth]{figures/chapter-2/learning-curves.png}
%\end{figure}




\section{Deep Learning}
\subsection{Introduction}
Deep learning refers to a field of machine learning that is based on the use of so called \emph{deep neural networks}. 

The approach is inspired by how the human brain works by modelling biological neurons and their interconnections to other neurons as terms of an artificial neuron that has similar properties. 

The details about our understanding of the inner mechanics and biochemical processes of the human brain are beyond the scope of this thesis. On a high level, however, Each neuron receives inputs signals on its dendrites which are connected to neighbouring neurons axons using a synapse. The inputs are accumulated and processed within the cell body causing the cell to output a signal on its own axon which in turn represents the potential input for other cells.
% TODO Refine this. Make sure this is correct and in the right section! What exactly is a synapse. 
Through these interconnections the neurons form a highly complex structure that can be conceptualised in terms of a biological neural network. It is estimated that the human brain possesses about 100 billion allowing it to process complex signals, abstract concepts, and the general process people refer to as \emph{thinking}. 
% TODO What about learning? 

In a bionic fashion an artificial neural network adopts this architecture in a simplified way by defining a network of artificial neurons that are connected according to a certain topology. The analogy between the human and the artificial neurons is illustrated in figure \ref{fig:biological-vs-artificial-neuron}. 
This way, an artificial neural network (henceforth called neural network), is able to capture complex correlations and interdependencies within the data. 

% GRAPH Recreate graph
\begin{figure}[h]
	\caption{A biological neuron vs. an artificial neuron}\label{fig:biological-vs-artificial-neuron}
	% GRAPH-LABEL Add proper description
	\centering
	\includegraphics[width=0.8\textwidth]{figures/chapter-2/biological-vs-artificial-neurons.png}
\end{figure}


The concepts of using neural networks has been in existence since the 1970s. % CITE first neural networks
After an initial enthusiasm, however, neural networks lost traction in the following two decades due to the realisation that the existing hardware did not allow for a degree of scalability that would have been required to solve the desired problems. % CITE ai-winter

This changed, however, when in 2014 X,Y, and Z used a neural network outperformed all existing methods on the \emph{ImageNet Challenge} which seeks to recognise and label a set of objects in pictures. % CITE imagenet

Ever since, deep neural networks have been responsible for some of the most recent successes including self-driving cars and defeating the world champion in Go. % CITE Go 
According to former chief data scientist and researcher at Stanford University, Andrew Ng, % CITE Andrew Ng 
two key factors are responsible for renaissance of neural networks and have enabled its recent success: Firstly, the advances in computational capacities which includes highly-optimised processing units such as GPUs and ASICS % CHECK ASICS? 
and corresponding architectures such as distributed clusters and cloud computing. Secondly, the availability of large datasets that can be used for training the models such as web-scale text corpora for natural language processing or large databases of images (imagenet) etc. 

Today, deep neural networks represent the state-of-the-art in many areas such as natural language processing % CITE NLP
and computer vision. They are widely considered one of the most promising areas of machine learning and artificial intelligence in general % CITE importance of deep learning
and have received a high level of attention in society, media, and politics. Despite its recent success, the usage of deep neural networks poses a multitude of computational, architectural, and domain-specific challenges % CITE Do I need to cite this? 
and therefore represents of most dynamic areas of research. 

\subsection{The Multilayer Perceptron}
In order to understand how neural networks work, we first investigate one of the most basic forms of neural networks: the \emph{multilayer perceptron} or MLP. 
% TODO Is a perceptron the same thing as a neuron? Are there other kinds of neurons? 
% GRAPH Redo graph
\begin{figure}[H]

\caption{Prototypical multilayer perceptron with a single hidden layer.}\label{fig:multilayer-perceptron}
% GRAPH-LABEL Add proper description
\centering
\includegraphics[width=0.6\textwidth]{figures/chapter-2/multilayer-perceptron.png}
\end{figure}
Figure \ref{fig:multilayer-perceptron} depicts the schematics of an MLP. As the name suggests, it consists of multiple layers each containing a fixed number of artificial neurons that are interconnected exclusively by neurons of neighbouring layers. The first, so called \emph{input layer}, is followed by one or more hidden layers leading to a final so called \emph{output layer}. 

\begin{figure}[h]
	\caption{Basic building block: Artificial Neuron used for classification.}\label{fig:artificial-neuron}   
	\centering
	\includegraphics[width=0.6\textwidth]{figures/chapter-2/artificial-neuron.png}
\end{figure}


The individual artificial neuron or unit is illustrated in figure \ref{fig:artificial-neuron}. Its output $y$ is computed as 
\begin{equation}
	% MATHS Check, looks weird with b in front
	y = \phi(b + \sum_{i=1}^{n} w_i x_i)
\end{equation}
where $x_1, \ldots, x_n \in \mathbb{R}$ corresponds to the inputs of the neuron, $w_1, \dots, w_n\in \mathbb{R}$ to the weights and $b \in \mathbb{R}$ to a bias -- trainable parameters of the model -- and $\phi: \mathbb{R} \rightarrow \mathbb{R}$ to a non-linear function referred to as \emph{activation function}. Typical choices for $\phi$ include 
\begin{align*} 
\phi(x) = \sigma(x) = \frac{1}{1 + e^{-x}} && \text{or} && \phi(x) = tanh(x) = \frac{1  - e^{-2x}}{1  + e^{-2x}}
\end{align*} 
where $\sigma(x)$ refers to the \emph{sigmoid function}, $tanh(x)$ to the \emph{hyperbolic tangent}, and 
% TODO Add another activation function
... to Z. While each of these activation functions has different properties as illustrated in figure \ref{fig:activation-functions}, a typical characteristic is that they map the input value to the closed interval $[0, 1]$. 

\begin{figure}[h]
	\caption{Typical Activation functions}\label{fig:activation-functions}   
	\centering
	\includegraphics[width=0.6\textwidth]{figures/chapter-2/activation-functions.png}
\end{figure}


% TODO What else do I need to write about MLPs?  Equations for the outcome? 
%Given this definition of a single unit, the final value $y$ of the outcome layer can be formalised as the concatenation of the different activation functions forward-propagated from the input layer to the output layer. For the MLP depicted in figure \ref{fig:multilayer-perceptron}, this means.
%
%\begin{align}
%x = 
%\end{align}

Despite the relative simplicity of this model, it can be shown that an MLP with appropriate weights and bias parameters is able to represent any arbitrarily complex non-linear function.% CITE Who showed this? Cybenko!

\subsection{Types of Neural Networks}
There are different types of neural networks defined by a number of characteristics such as network's architecture and the direction of information flow. 

% QUESTION: What's the difference between MLP and standard ff nn? 
\paragraph{Feed-Forward Neural Networks} Closely related to the MLP described in the previous section, feed-forward neural networks (FFNN) represent a class of networks that is characterised by a set of hidden layers that have a similar shape and are often fully connected (i.e. every node in layer $L_i$ is connected to every node in layer $L_{i-1}$ and $L_{i+1}$). As the name suggest, the information flow is strictly uni-directional from nodes in layers with lower indexes to nodes with higher indexes (i.e. no loops). These types of networks represent the most basic type of network that makes no assumptions about the input data and is used in regression and classification tasks.  

\paragraph{Convolutional Neural Networks} While FFNNs make no assumptions about the input data whatsoever, it is often useful to exploit domain-specific knowledge about the specific input data. For instance, in computer vision the input of a network is typically an image encoded as pixmap with the intensity values of each pixel. In this case, it seems naive and inefficient to assume independence of the inputs ignoring aspects like the principle of locality of neighbouring pixels which are more likely to have a similar colour or intensity than two randomly-selected pixels. 
% ALT: In contrast to FFNNs which make no assumption about the input whatserver, Convnets are designed to exploit domain-specific knowledge. In computer vision for instance, we can ... 
\begin{figure}[h]
	\centering
	\includegraphics[width=1\textwidth]{figures/chapter-2/convnet.png}
	\caption{Convolutional Neural Network}\label{fig:convnet}   
\end{figure}
A convolutional neural network or ConvNet is a special kind of feed-forward neural network whose architecture is designed to exploit the principle of locality in the data. This is typically achieved by alternating \emph{convolutional layers} and \emph{pooling layers}. The convolutional layers run a filter or \emph{kernel} across the their input layer which performs an image convolution and can be thought of a way to detect features (such as edges) in the image. The pooling layers typically perform some kind of aggregation (such as taking the maximum of multiple values) over the previous layer to reduce the dimensionality. 

While convnets are particularly useful in computer vision and represent the state-of-the-art in image classification % CITE imagenet
, they can also be applied in other fields in which the data expresses some principle of locality. 

\paragraph{Recurrent Neural Networks} In contrast to FFNNs and convnets for which the information flow is strictly uni-directional, recurrent neural networks (RNNs) are characterised by some kind of feedback loop that allows the outputs of a unit in layer $L_i$ to be process as input by any other layer $L_j$, even if $i \geq j$.

This allows the network to keep an internal state which can be conceptualised as memory. Such a memory enables the network to effectively deal with sequences of data such as time-series values, natural language, or even music. 


\begin{wrapfigure}{r}{0.4\textwidth}
	\centering
	\includegraphics[width=0.4\textwidth]{figures/chapter-2/lstm.png}
	\caption{LSTM}\label{fig:lstm}   
\end{wrapfigure}

In spite of the benefits the recurrent architecture of the network provides, it also introduces a number of computational challenges such as the so called \emph{vanishing gradient problem} which stems from the additional distance the gradient is backpropagates (see next section % TODO Reconsider training section. Should it come before? 
) in the computation graph of the network causing the gradient. In order to circumvent this problem, a number architectures has been proposed for an individual cell within the RNN, most notably \emph{long short-term memory} (LSTM) cells and \emph{gated recurrent units} (GRUs). As illustrated in figure \ref{fig:lstm} an LSTM achieves this using a number of \emph{gates} that allow the cell to decide when to store and when to reset (i.e. forget) its internal state. 
% TODO Explain this properly. Make sure I fully understand it myself. ;) 

Today for many problems in machine learning, RNNs represent the state-of-the-art when dealing with sequential data such as natural language and time-series. 






\subsection{Training}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Multi-Task Learning}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Hyper-paramaters}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Challenges}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.


\section{Counterfactual Inference}
\subsection{Motivation}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Formalisation}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Counterfactual Inference vs. Supervised Learning}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Propensity Score}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Status Quo}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

\subsection{Open Questions}
Lorem ipsum dolor sit amet consectetur, adipiscing elit mus neque montes, suspendisse et sociis vestibulum.

