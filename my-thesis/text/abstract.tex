The technological advances of recent years have resulted in an increasing availability of data in various fields such as healthcare, education, and economics. Of particular interest are observational studies which investigate the effect of a specific intervention or action on a given context or subject. 

When inferring causal relations from the data, we are often interested in answering counterfactual questions such as ”What would have happened if a different action had been taken?” allowing us to estimate the \emph{individualised treatment effect} -- an important quantity representing the difference between the potential outcomes.

Due to the very nature of the problem, we are never able to observe both potential outcomes in any real-world dataset. Furthermore, the factual and counterfactual feature distributions are typically different making counterfactual inference a challenging task that is inherently different from regular supervised learning. 

In the scope of this thesis we investigate how deep neural networks can be used for the task of counterfactual inference. This class of models is able to capture complex non-linear dependencies in the data and responsible for a number of recent successes in problem areas such as computer vision and natural language processing. 

Concretely, we propose \emph{deep counterfactual networks} (DCNs), a novel approach that conceptualises causal inference as a multi-task learning problem. Furthermore, we introduce \emph{propensity-dropout} (PD), a corresponding regularisation scheme based on traditional dropout.

In addition, we present an efficient method for automatically inferring appropriate hyper-parameters for DCNs based on an empirical model.

We conduct experiments based on synthetic datasets and two real-world observational studies and conclude by showing how our models have the potential to compete with and even outperform the state-of-the art in counterfactual inference. 


